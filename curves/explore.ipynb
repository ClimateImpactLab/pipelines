{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import metacsv\n",
    "import csv\n",
    "import StringIO\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "import copy\n",
    "from glob import glob\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Users load a config file with three parameters\n",
    "        1. mode: single, mc, or median. Single seems like it is totally dry-run just to get basic feedback\n",
    "            A. These refer to number of realizations of full run to do\n",
    "        2. Specification: Mortality, Labor, Ag, whatever impact\n",
    "        3. Output dir, where to write this stuff\n",
    "        \n",
    "2. This config file is read by generate.generate and calls the specific functions in that file. \n",
    "\n",
    "3. Each of the three options call functions that yield the inputs that are then called for the specific impact function that is being specified. Each of these impact sectors is uniquely specified in another file but the interface to the impact functions are all the same.\n",
    "\n",
    "4. Each discrete calculation regardless of it is single or mc or median need the following inputs\n",
    "        pvals: the pvalue to sample at \n",
    "        clim_scenario: RCP #file identification\n",
    "        clim_model: CMIP5 model: file identification\n",
    "        weatherbundle: weather values of tas, pr, wind, etc: file identification\n",
    "        econ_scenario: high or low# config\n",
    "        econ_model: SSSP# config\n",
    "        economicmodel: Wraps a class around the data that comes from econ_scenario and econ_model with access to basic stats about the economic model. Not sure how to get this out but maybe it is simple do_econ_thing decorator\n",
    "        \n",
    "        \n",
    "\n",
    "Behind this is the whole set up of the entire flow of data that unleashes the whole calculation. When get_bundle_iterator is called it goes and queues up all the necessary values for the computation and the generators emit the information one by one. So generate.generate is more infrastructure and input file config. And it sets that config up based on the type of calculation you are runnning. \n",
    "\n",
    "Questions\n",
    "What I don't totally understand is binpush_callback, valpush_callback, binresult_callback, valresult_callback, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "If the goals of the gcp is to estimate/forecast future joint probability distributions, then the we need to represent those \n",
    "distributions as data structures. We need to represent a joint probability distribution for every location for every year\n",
    "in our dataset. Each of those probability distributions is the result of a number of uncertain/probabalistic inputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pvals, clim_scenario, clim_model, weatherbundle, econ_scenario, econ_model, economicmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs_histclim = xr.open_dataset('/Users/rhodiumgroup/data/gcp_stuff/labor_global_interaction_2factor_best_14feb-histclim.nc4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_labor = xr.open_dataset('/Users/rhodiumgroup/data/gcp_stuff/labor_global_interaction_2factor_best_14feb.nc4')\n",
    "output_labor_c = xr.open_dataset('/Users/rhodiumgroup/data/gcp_stuff/labor_global_interaction_2factor_best_14feb_comatose.nc4')\n",
    "output_labor_d = xr.open_dataset('/Users/rhodiumgroup/data/gcp_stuff/labor_global_interaction_2factor_best_14feb_dumb.nc4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frozen(OrderedDict([(u'year', <xarray.IndexVariable u'year' (year: 120)>\n",
       "array([1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992,\n",
       "       1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004,\n",
       "       2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016,\n",
       "       2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028,\n",
       "       2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040,\n",
       "       2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052,\n",
       "       2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064,\n",
       "       2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076,\n",
       "       2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088,\n",
       "       2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100], dtype=int32)\n",
       "Attributes:\n",
       "    long_title: Impact year (Gregorian calendar)\n",
       "    units: Years\n",
       "    source: From the weather file.), (u'regions', <xarray.Variable (region: 24378)>\n",
       "array([u'CAN.1.2.28', u'CAN.1.17.403', u'CAN.2.34.951', ..., u'BWA.6.16',\n",
       "       u'BWA.5', u'BWA.4.13'], dtype=object)\n",
       "Attributes:\n",
       "    long_title: Region ID\n",
       "    units: None\n",
       "    source: From the hierarchy file.), (u'rebased', <xarray.Variable (year: 120, region: 24378)>\n",
       "[2925360 values with dtype=float64]\n",
       "Attributes:\n",
       "    long_title: Rebased Sum of previous results\n",
       "    units: portion\n",
       "    source: The result calculated relative to the year 2005, by re-basing variable sum.), (u'sum', <xarray.Variable (year: 120, region: 24378)>\n",
       "[2925360 values with dtype=float64]\n",
       "Attributes:\n",
       "    long_title: Sum of previous results\n",
       "    units: minutes worked by individual\n",
       "    source: Sum of Direct marginal response, Direct marginal response, Direct marginal response), (u'response', <xarray.Variable (year: 120, region: 24378)>\n",
       "[2925360 values with dtype=float64]\n",
       "Attributes:\n",
       "    long_title: Direct marginal response\n",
       "    units: minutes worked by individual\n",
       "    source: The average result across a year of daily temperatures applied to a polynomial.), (u'response2', <xarray.Variable (year: 120, region: 24378)>\n",
       "[2925360 values with dtype=float64]\n",
       "Attributes:\n",
       "    long_title: Direct marginal response\n",
       "    units: minutes worked by individual\n",
       "    source: The average result across a year of daily temperatures applied to offset to normalize to 27 degrees), (u'response22', <xarray.Variable (year: 120, region: 24378)>\n",
       "[2925360 values with dtype=float64]\n",
       "Attributes:\n",
       "    long_title: Direct marginal response\n",
       "    units: minutes worked by individual\n",
       "    source: The average result across a year of daily temperatures applied to effect from days less than 0 C)]))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Empty Attributes>, <Empty Coordinates>, <Empty Variables>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metacsv.read_header('/Users/rhodiumgroup/data/gcp_stuff/gdppc-merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use some decorators or closures to deal with inputs that are custom for each run\n",
    "#\n",
    "\n",
    "def do_climate_thing(path, climate_arguments):\n",
    "    '''\n",
    "    Load climate data year by year, we need tasmax by IR. We have this data. \n",
    "    Per run, do this 100 times, read a new dataset in, and take the dot-product\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        path to a netCDF file\n",
    "        \n",
    "    climate_arguments: str\n",
    "        climate variable evaluated \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataArray\n",
    "\n",
    "    '''  \n",
    "    ds = xr.open_dataset(path)\n",
    "    return ds[climate_arguments]\n",
    "    \n",
    "    \n",
    "    \n",
    "def do_covariate_thing(*paths):\n",
    "    '''\n",
    "    Read in covariate data\n",
    "    Align datasets along region and time dimension\n",
    "    Perform any necessary transformations\n",
    "    For socio-economic data we need to reevaluate every few years \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Paths to covariate data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    MxN Matrix where M is the number of IRs and N is the number of covariates\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #read file\n",
    "    #take baselines\n",
    "    #update parameters\n",
    "    \n",
    "        \n",
    "def get_local_surface_estimate_at_given_p_value(IR_annual_covariates, gammas, pval):\n",
    "    '''\n",
    "    Element-wise multiplication of gammas and IR-level covariates. \n",
    "    Equivalent to the local response functions\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    IR_covariates: MxN matrix where M is the num IRs and N is the covariates\n",
    "    \n",
    "    gammas: Mx1 column vector of point estimates of global impact function specification parameters\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    MxN Matrix where M is IRs and N is local estimate of function specification params\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #Load IR matrix\n",
    "    #load gammas\n",
    "    #do math\n",
    "    #return matrix maybe write to disk or do some diagnostic\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def do_math_thing(climate_object, IR_parameters,function_specification):\n",
    "    '''\n",
    "    Do math according to function specification\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Climate_object: Xarray DataSet or Xarray DataArray\n",
    "    \n",
    "    IR_parameters: MxN matrix of regions by parameters \n",
    "    \n",
    "    function_specification: Arbitrary function that takes Climate_object and IR_params as args\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Xarray object: result of computation as a MxN Matrix where M is the number of IRs and N is number responses\n",
    "                   represents one year of impacts for each IR\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return function_specification(climate_object, IR_parameters)\n",
    "\n",
    "def do_collect_results_thing(annual_responses_by_IR_object, temp_path_on_disk, output_dir=None):\n",
    "    '''\n",
    "    Collects annual results into larger dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    annual_responses_by_IR_object: Xarray DataSet\n",
    "        represents annual impacts\n",
    "        \n",
    "    temp_path_on_disk\n",
    "    -----------------\n",
    "        \n",
    "    output_dir: str\n",
    "        Location to write output\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    N-dimensional data set or None\n",
    "    \n",
    "    '''\n",
    "    #append annual_responses_by_IR_object to file on disk\n",
    "    ds = xr.open_dataset(path_on_disk)\n",
    "    #ds.update(annual_response_by_IR_object) \n",
    "    \n",
    "    if output_dir is not None:\n",
    "        ds.write_to_netcdf(output_dir)\n",
    "    return ds\n",
    "    \n",
    "\n",
    "def to_datafs(api_object, archive_name, ds= None,path=None, cache=False)\n",
    "    '''\n",
    "    Creates a record in datafs and uploads file to osdc. If cache is True will save to cache\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    api_object: initialized api with users parameters from local datafs config file\n",
    "    \n",
    "    archive_name: str\n",
    "        name of archive to create in datafs\n",
    "        \n",
    "    ds: Xarray Object\n",
    "        dataset to be created and uploaded to datafs \n",
    "    \n",
    "    path: str\n",
    "        if None then archive is created from memory\n",
    "    \n",
    "    cache=bool\n",
    "        if True, save file to cache\n",
    "    \n",
    "    '''\n",
    "\n",
    "def do_pipeline(init_dict):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    init_dict: dict\n",
    "        Specifies the parameters of the subroutines in do_pipeline  \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    Creates archive(s) in datafs that correspond to parameters in init_dict\n",
    "    \n",
    "    \n",
    "    1.get coefficients from the csvv draw via np.multinomial.rvs(gammas, covariates)\n",
    "    2.multiply coefficients by values in socioeconomics for each region \n",
    "    3.when we multiply the IR-level socio values and the gammas we get an IR-level curve\n",
    "    4.take the dot product of the climate variable at that order of magnitude \n",
    "    '''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "degree_days = xr.open_dataset('/Users/rhodiumgroup/data/gcp_stuff/Degreedays_aggregated_rcp45_r1i1p1_CCSM4.nc')\n",
    "\n",
    "coldd = degree_days.coldd_agg\n",
    "hotdd = degree_days.hotdd_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'coldd_agg' (time: 119, SHAPENUM: 24378)>\n",
       "[2900982 values with dtype=float32]\n",
       "Coordinates:\n",
       "  * SHAPENUM  (SHAPENUM) int32 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ...\n",
       "Dimensions without coordinates: time\n",
       "Attributes:\n",
       "    long_title: aggregation ofCold degree days that tasmax is less than 10 degree celsuisin impact regions\n",
       "    units: degree days\n",
       "    source: Regional aggregated coldd from /global/scratch/jiacany/nasa_bcsd/Labor/degree_days/tasmax/rcp45/CCSM4/tasmax_exceedance_degree_days_rcp45_r1i1p1_CCSM4_1.nc"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coldd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tasmax = glob('/Users/rhodiumgroup/data/gcp_stuff/tasmax_day_aggregated_rcp45_r1i1p1_CCSM4_2006.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_baseline = pd.read_csv('/Users/rhodiumgroup/data/gcp_stuff/popop_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '/Users/rhodiumgroup/data/gcp_stuff/labor_global_interaction_3factor_BEST_14feb.csvv'\n",
    "csvv = read(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:   (SHAPENUM: 24378, time: 365)\n",
       "Coordinates:\n",
       "  * SHAPENUM  (SHAPENUM) int32 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ...\n",
       "  * time      (time) float32 2.006e+06 2.006e+06 2.006e+06 2.006e+06 ...\n",
       "Data variables:\n",
       "    tasmax    (time, SHAPENUM) float32 -10.7914 -10.6006 0.773431 -12.0515 ...\n",
       "Attributes:\n",
       "    version: BCSD.2016-02-21\n",
       "    description: GCP regional agrregated data\n",
       "    dependencies: Agglomerated-Many.2016-02-17NASA-GDDP\n",
       "    contact: Jiacan Yuan (jiacan.yuan@gmail.com), Rutgers University"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasmax = xr.open_dataset('/Users/rhodiumgroup/data/gcp_stuff/tasmax_day_aggregated_rcp45_r1i1p1_CCSM4_2006.nc')\n",
    "tasmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collapse_bang(csvv, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subs = subset(csvv, prednames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotdd_agg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHAPENUM</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          hotdd_agg\n",
       "SHAPENUM           \n",
       "0               2.0\n",
       "1               5.0\n",
       "2               0.0\n",
       "3               0.0\n",
       "4               0.0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdd = hotdd[0].to_dataframe()\n",
    "hdd.index -= 1\n",
    "hdd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gdppc</th>\n",
       "      <th>hierid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20631</td>\n",
       "      <td>CAN.1.2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8329</td>\n",
       "      <td>CAN.1.17.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2101</td>\n",
       "      <td>CAN.2.34.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17090</td>\n",
       "      <td>CAN.11.259.4274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13741</td>\n",
       "      <td>CAN.11.269.4448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gdppc           hierid\n",
       "0  20631       CAN.1.2.28\n",
       "1   8329     CAN.1.17.403\n",
       "2   2101     CAN.2.34.951\n",
       "3  17090  CAN.11.259.4274\n",
       "4  13741  CAN.11.269.4448"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdppc = pd.DataFrame(np.random.randint(1000,23000, 24378))\n",
    "gdppc['hierid'] = popop['hierid']\n",
    "gdppc.columns = ['gdppc', 'hierid']\n",
    "gdppc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popop = pop_baseline[['hierid', 'popop']]\n",
    "popop.head()\n",
    "merged = pd.merge(popop, gdppc, on='hierid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'coldd_agg' (SHAPENUM: 24378)>\n",
       "array([ 2312.,  2076.,  1161., ...,     0.,     0.,     0.], dtype=float32)\n",
       "Coordinates:\n",
       "  * SHAPENUM  (SHAPENUM) int32 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ...\n",
       "Attributes:\n",
       "    long_title: aggregation ofCold degree days that tasmax is less than 10 degree celsuisin impact regions\n",
       "    units: degree days\n",
       "    source: Regional aggregated coldd from /global/scratch/jiacany/nasa_bcsd/Labor/degree_days/tasmax/rcp45/CCSM4/tasmax_exceedance_degree_days_rcp45_r1i1p1_CCSM4_1.nc"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged['hotdd'] = hotdd[0]\n",
    "merged['coldd'] = coldd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.06338691e+02,   2.06310000e+04,   2.00000000e+00,\n",
       "         2.31200000e+03])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for every year/region calculation you'd have to create something like this to get local\n",
    "#Should we create a data structure that is just IR-levelbetas\n",
    "merged_new  = merged.ix[:,1:5]\n",
    "merged_new.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IR_curves= []\n",
    "for k,v in predgammas.items():\n",
    "    IR_curve = {}\n",
    "    for i in merged_new.values[0]:\n",
    "        IR_curve[i] = {}\n",
    "        IR_curve[i][k] = merged_new.values*v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2.0: {'tasmax2': array([[ -7.32877888e+02,  -3.49285426e+03,   2.05635875e+00,\n",
       "            3.08205891e+02],\n",
       "         [ -1.24345787e+02,  -1.41011018e+03,   5.14089687e+00,\n",
       "            2.76745428e+02],\n",
       "         [ -1.30527697e+01,  -3.55701944e+02,   0.00000000e+00,\n",
       "            1.54769481e+02],\n",
       "         ..., \n",
       "         [ -1.68765919e+03,  -7.99609843e+02,   4.25955248e+02,\n",
       "            0.00000000e+00],\n",
       "         [ -6.65730516e+02,  -9.72466427e+02,   4.55237494e+02,\n",
       "            0.00000000e+00],\n",
       "         [ -7.44772535e+02,  -9.68233896e+02,   5.20143451e+02,\n",
       "            0.00000000e+00]])},\n",
       " 606.33869120000008: {'tasmax2': array([[ -7.32877888e+02,  -3.49285426e+03,   2.05635875e+00,\n",
       "            3.08205891e+02],\n",
       "         [ -1.24345787e+02,  -1.41011018e+03,   5.14089687e+00,\n",
       "            2.76745428e+02],\n",
       "         [ -1.30527697e+01,  -3.55701944e+02,   0.00000000e+00,\n",
       "            1.54769481e+02],\n",
       "         ..., \n",
       "         [ -1.68765919e+03,  -7.99609843e+02,   4.25955248e+02,\n",
       "            0.00000000e+00],\n",
       "         [ -6.65730516e+02,  -9.72466427e+02,   4.55237494e+02,\n",
       "            0.00000000e+00],\n",
       "         [ -7.44772535e+02,  -9.68233896e+02,   5.20143451e+02,\n",
       "            0.00000000e+00]])},\n",
       " 2312.0: {'tasmax2': array([[ -7.32877888e+02,  -3.49285426e+03,   2.05635875e+00,\n",
       "            3.08205891e+02],\n",
       "         [ -1.24345787e+02,  -1.41011018e+03,   5.14089687e+00,\n",
       "            2.76745428e+02],\n",
       "         [ -1.30527697e+01,  -3.55701944e+02,   0.00000000e+00,\n",
       "            1.54769481e+02],\n",
       "         ..., \n",
       "         [ -1.68765919e+03,  -7.99609843e+02,   4.25955248e+02,\n",
       "            0.00000000e+00],\n",
       "         [ -6.65730516e+02,  -9.72466427e+02,   4.55237494e+02,\n",
       "            0.00000000e+00],\n",
       "         [ -7.44772535e+02,  -9.68233896e+02,   5.20143451e+02,\n",
       "            0.00000000e+00]])},\n",
       " 20631.0: {'tasmax2': array([[ -7.32877888e+02,  -3.49285426e+03,   2.05635875e+00,\n",
       "            3.08205891e+02],\n",
       "         [ -1.24345787e+02,  -1.41011018e+03,   5.14089687e+00,\n",
       "            2.76745428e+02],\n",
       "         [ -1.30527697e+01,  -3.55701944e+02,   0.00000000e+00,\n",
       "            1.54769481e+02],\n",
       "         ..., \n",
       "         [ -1.68765919e+03,  -7.99609843e+02,   4.25955248e+02,\n",
       "            0.00000000e+00],\n",
       "         [ -6.65730516e+02,  -9.72466427e+02,   4.55237494e+02,\n",
       "            0.00000000e+00],\n",
       "         [ -7.44772535e+02,  -9.68233896e+02,   5.20143451e+02,\n",
       "            0.00000000e+00]])}}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IR_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tasmax', 'tasmax4', 'tasmax3', 'tasmax2']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predgammas.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasmax': -33.912498866251397,\n",
       " 'tasmax2': 5.2971225130931705,\n",
       " 'tasmax3': -1.7344001597803684,\n",
       " 'tasmax4': -0.0058674426793554708}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasmax': ['loggdppc', 'logpopop', 'hotdd_agg', 'coldd_agg'],\n",
       " 'tasmax2': ['loggdppc', 'logpopop', 'hotdd_agg', 'coldd_agg'],\n",
       " 'tasmax3': ['loggdppc', 'logpopop', 'hotdd_agg', 'coldd_agg'],\n",
       " 'tasmax4': ['loggdppc', 'logpopop', 'hotdd_agg', 'coldd_agg']}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predcovars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_curve = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Curve_logic\n",
    "\n",
    "# The conceptual goal of interpolation-and-adaptation\n",
    "# process in the GCP is to estimate response curves at some unknown\n",
    "# location and future time.  So, those response curves are embodied as\n",
    "# instances of the Curve class.  Whatever that process is to generate\n",
    "# those response curves is embodied in a CurveGenerator class.\n",
    "\n",
    "\n",
    "#To get the inputs to this curve, we read in the csvv file take the gammas and then the diagonal of the covariance matrix\n",
    "\n",
    "\n",
    "def read(filename):\n",
    "    with open(filename, 'rU') as fp:\n",
    "        attrs, coords, variables = metacsv.read_header(fp, parse_vars=True)\n",
    "        data = {'attrs': attrs, 'variables': variables, 'coords': coords}\n",
    "\n",
    "        if 'csvv-version' in attrs:\n",
    "            if attrs['csvv-version'] == 'girdin-2017-01-10':\n",
    "                return read_girdin(data, fp)\n",
    "\n",
    "     \n",
    "def read_girdin(data, fp):\n",
    "    reader = csv.reader(fp)\n",
    "    variable_reading = None\n",
    "\n",
    "    for row in reader:\n",
    "        if len(row) == 0 or (len(row) == 1 and len(row[0].strip()) == 0):\n",
    "            continue\n",
    "\n",
    "        if row[0] in ['observations', 'prednames', 'covarnames', 'gamma', 'gammavcv', 'residvcv']:\n",
    "            data[row[0]] = []\n",
    "            variable_reading = row[0]\n",
    "        else:\n",
    "            if variable_reading is None:\n",
    "                print \"No variable queued.\"\n",
    "                print row\n",
    "            assert variable_reading is not None\n",
    "            data[variable_reading].append(map(lambda x: x.strip(), row))\n",
    "\n",
    "    data['observations'] = float(data['observations'][0][0])\n",
    "    data['prednames'] = data['prednames'][0]\n",
    "    data['covarnames'] = data['covarnames'][0]\n",
    "    data['gamma'] = np.array(map(float, data['gamma'][0]))\n",
    "    data['gammavcv'] = np.array(map(lambda row: map(float, row), data['gammavcv']))\n",
    "    data['residvcv'] = np.array(map(lambda row: map(float, row), data['residvcv']))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collapse_bang(data, seed):\n",
    "    if seed == None:\n",
    "        data['gammavcv'] = None\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "        data['gamma'] = multivariate_normal.rvs(data['gamma'], data['gammavcv'])\n",
    "        data['gammavcv'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subset(csvv, prednames):\n",
    "    toinclude = map(lambda predname: predname in prednames, csvv['prednames'])\n",
    "    toinclude = np.where(toinclude)[0]\n",
    "\n",
    "    subcsvv = copy.copy(csvv)\n",
    "    subcsvv['prednames'] = [csvv['prednames'][ii] for ii in toinclude]\n",
    "    subcsvv['covarnames'] = [csvv['covarnames'][ii] for ii in toinclude]\n",
    "    subcsvv['gamma'] = csvv['gamma'][toinclude]\n",
    "    if 'gammavcv' in csvv and csvv['gammavcv'] is not None:\n",
    "        subcsvv['gammavcv'] = csvv['gammavcv'][toinclude, toinclude]\n",
    "\n",
    "    return subcsvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prednames = ['tasmax', 'tasmax2', 'tasmax3', 'tasmax4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = 'C'\n",
    "dep = 'minutes worked by individual', \n",
    "prefix = 'tasmax' \n",
    "order = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#subs['gamma'] = gamma\n",
    "#subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "constant = {} # {predname: constant}\n",
    "predcovars = {} # {predname: [covarname]}\n",
    "predgammas = {} # {predname: np.array}\n",
    "for predname in set(prednames):\n",
    "    constant[predname] = 0\n",
    "    predcovars[predname] = []\n",
    "    predgammas[predname] = []\n",
    "    \n",
    "    indices = [ii for ii, xx in enumerate(subs['prednames']) if xx == predname]\n",
    "    for index in indices:\n",
    "        if subs['covarnames'][index] == '1':\n",
    "                    constant[predname] += subs['gamma'][index]\n",
    "        else:\n",
    "            predcovars[predname].append(subs['covarnames'][index])\n",
    "            predgammas[predname].append(subs['gamma'][index])\n",
    "    predgammas[predname] = np.array(predgammas[predname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_coefficients(covariates, debug=False):\n",
    "    coefficients = {} # {predname: sum}\n",
    "    for predname in set(prednames):\n",
    "        if len(predgammas[predname]) == 0:\n",
    "            coefficients[predname] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                coefficients[predname] = constant[predname] + np.sum(predgammas[predname] * np.array([covariates[covar] for covar in predcovars[predname]]))\n",
    "            except Exception as e:\n",
    "                print \"Available covariates:\"\n",
    "                print covariates\n",
    "                raise e\n",
    "\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read from gammas and associated covariance matrix. \n",
    "# X Mathematical operations: \n",
    "    #one: When calling get_curve for the FarmerCurveGenerator you are actually calling \n",
    "    # np.polyval\n",
    "\n",
    "    #one:line 46 of curvegen.CSVVCurvegen: \n",
    "            #1. Defines the calculation for each coefficient according to the type of distribution that will be computed\n",
    "            #2. Reads the csvv and sets up the datastructure\n",
    "            #3. I don't see where the numerica values from the csvv are computed until\n",
    "        #Generates a coefficient for each predictor so we'll have a dict\n",
    "        #Is this a set of predictor: coefficient key-value pairs for each region, for each year\n",
    "        #{'tasmax': someval, 'tasmax2': someval, 'tasmax3': some_val, 'tasmax4': some_val}\n",
    "        #Each coefficient (tasmax, tasmax2, tasmax3 etc) is the sum of some constant and the \n",
    "        #product of that tasmax*loggdp + tasmax*hotdd + tasmax*coldd\n",
    "    #two: Then call this Farmer curve generator which basically takes the \n",
    "        #dictionary of coefficients and the computation that generates them adds them to a list\n",
    "        # For each region there is this list of dictionaries which represent the a specified calculation \n",
    "        # coefficient on the predictor climate variable.\n",
    "        #Then that is handed to np.polyval\n",
    "        #Can you point me to a get_cruve method that is implemented? I'd like to understand the \n",
    "        #the operation that specifies the curve. \n",
    "        #Region Curves: List of curves defined by region specific output of np.polyval\n",
    "#Generate a curve for Farmer: \n",
    "#ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "When we get to curve generation there are two module-level curve generation calls\n",
    "1. PolynomialCurveGeneratro(or MLECubicSplineGenerator), this takes in the modified data structure that came from the csvv\n",
    "subcsvv now is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finals = xr.open_dataset('/Users/rhodiumgroup/data/gcp_stuff/labor_global_interaction_3factor_best_14feb.nc4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'response' (region: 24378)>\n",
       "array([  44.209047,   76.841777,   40.760857, ...,  237.901736,  292.757717,\n",
       "        316.413947])\n",
       "Coordinates:\n",
       "    year     int32 1981\n",
       "Dimensions without coordinates: region\n",
       "Attributes:\n",
       "    long_title: Direct marginal response\n",
       "    units: minutes worked by individual\n",
       "    source: The average result across a year of daily temperatures applied to a polynomial."
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
